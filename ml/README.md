# DSS Machine Learning Skeleton

This directory houses every artifact required to move from historical data to a production-grade DSS recommendation model once datasets become available.

## Folder Map
- `data/` – raw exports and cleaned feature tables. Keep sensitive files out of version control.
- `notebooks/` – exploratory analysis, feature prototyping, and experiment tracking.
- `scripts/` – reproducible pipelines (training, evaluation, data preparation).
- `serving/` – lightweight inference harnesses, ONNX converters, and HTTP/CLI adapters.
- `artifacts/` – packaged models, metrics, and schema snapshots generated by the scripts.

## Suggested Workflow
1. Export and anonymize historical records (feed, mortality, egg) into `data/raw/`.
2. Use the notebook template to explore data quality, engineer features, and lock target definitions.
3. Promote the finalized logic into `scripts/train_pipeline.py`, then version metrics alongside artifacts.
4. Drop the chosen artifact into `artifacts/` and update the serving stub to load it.
5. Wire the Laravel endpoint (`/api/ml/dss/predict`) to the real inferencer when ready.

> Tip: keep experiment notes (parameters, metrics) in the notebook or push them into your preferred tracking tool so retraining remains auditable.
